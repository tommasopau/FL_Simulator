import torch
import logging
from utils.seeding import set_deterministic_mode
from utils.logger import setup_logger
setup_logger() 
from pydantic import ValidationError
from utils.models import MNISTCNN, FCMNIST , ZalandoCNN , FNet , AdultCensusIncomeClassifier , CovertypeClassifier, KDDSimpleNN

from utils.constants import ALLOWED_FILTERS , MODEL_MAPPING
from dataset.dataset import FederatedDataLoader, DatasetHandler, DatasetHandlerTab, server_dataset
from server import Server, AggregationStrategy, AttackServer, AttackType , FLTrust
import multiprocessing
import ray
from flask import Flask, jsonify, request
from utils.config import Config, load_config , serialize_config , validate_config
from utils.db import get_engine, create_tables, get_session, SimulationResult
from sqlalchemy import and_, or_, Boolean , inspect
import json


def main():
    # Initialize Ray
    ray.init()
    try:
        
        
        # Initialize centralized logger
            
        logger = logging.getLogger(__name__)
        logger.info("Main process started.")
        
        set_deterministic_mode(federated_cfg.get('seed', 42))    
        
        config = load_config('config.yaml')
        config = serialize_config(config)
        validate_config(config)
            
        federated_cfg = config['federated_learning']
        model_cfg = config['model']
            
        
            
        # Verify device availability
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"Using device: {device}")
            
        
        model_type = model_cfg.get('type', 'FCMNIST')
        model_class = MODEL_MAPPING.get(model_type , None)
        if model_class is None:
            logger.error(f"Unsupported model type: {model_type}")
            raise ValueError(f"Unsupported model type: {model_type}")
            
        global_model = model_class().to(device)
        logger.info(f"Initialized global model: {model_type}")
            
        # Determine the appropriate DatasetHandler based on the dataset type
        dataset_id = federated_cfg['dataset'].lower()
        if dataset_id not in ['mnist', 'fashion_mnist','uoft-cs/cifar10']:
            dataset_handler_class = DatasetHandlerTab
        else:
            dataset_handler_class = DatasetHandler

        federated_dataset_handler = dataset_handler_class(
            datasetID=federated_cfg['dataset'],
            num_clients=federated_cfg['num_clients'],
            partition_type=federated_cfg['partition_type'],
            alpha=federated_cfg['alpha'],
            seed=federated_cfg.get('seed', 42),
        )
        if federated_cfg['attack'].upper() == 'LABEL_FLIP':
            federated_dataset_handler.label_flipping_attack = True
            federated_dataset_handler.num_attackers = federated_cfg['num_attackers']
        federated_dataset_handler._initialize_partitioner()
        federated_dataset_handler.load_federated_dataset()    
        # Initialize FederatedDataLoader
        federated_data_loader = FederatedDataLoader(
            dataset_handler=federated_dataset_handler,
            batch_size=federated_cfg['batch_size'],
            device=device
        )
        
        
            
        attack_str = federated_cfg.get('attack', None)
            
        attack_type = AttackType[attack_str.upper()]
        
        if federated_cfg['aggregation_strategy'].upper() == 'FLTRUST':
            server_data_loader = federated_dataset_handler.server_dataset(batch_size=federated_cfg['batch_size'])
            # Instantiate FLTrust server.
            server = FLTrust(
                server_data_loader=server_data_loader,
                attack_type=attack_type,
                federated_data_loader=federated_data_loader,
                global_model=global_model,
                aggregation_strategy=AggregationStrategy.FLTRUST,
                sampled=federated_cfg['sampled_clients'],
                global_epochs=federated_cfg['global_epochs'],
                local_epochs=federated_cfg['local_epochs'],
                learning_rate=federated_cfg['learning_rate'],
                batch_size=federated_cfg['batch_size'],
                local_dp=federated_cfg['local_DP_SGD'],
                device=device,
                f=federated_cfg['num_attackers'],
            )
        else:
            aggregation_strategy = federated_cfg.get('aggregation_strategy')
            aggregation_strategy = AggregationStrategy[aggregation_strategy]
            server = AttackServer(
                attack_type=attack_type,
                federated_data_loader=federated_data_loader,
                global_model=global_model,
                aggregation_strategy=aggregation_strategy,
                sampled = federated_cfg['sampled_clients'],
                global_epochs=federated_cfg['global_epochs'],
                local_epochs=federated_cfg['local_epochs'],
                learning_rate=federated_cfg['learning_rate'],
                batch_size=federated_cfg['batch_size'],
                local_dp = federated_cfg['local_DP_SGD'],
                device=device,
                f = federated_cfg['num_attackers'],
                    
            )

        # Run federated training and emit updates
        accuracy = server.run_federated_training()
        try:
            result_filepath = "results.txt"  # change to absolute path if needed
            with open(result_filepath, "a") as result_file:
                result_file.write("Simulation Result\n")
                result_file.write(f"Accuracy: {accuracy}\n")
                result_file.write("Configuration:\n")
                result_file.write(json.dumps(config, indent=4))
            logger.info(f"Results saved to {result_filepath}")
            
            # New: write epoch results along with the config into another file (append mode)
            epoch_filepath = "simulation_epoch_results.txt"
            with open(epoch_filepath, "a") as epoch_file:
                epoch_file.write("Simulation Epoch Results\n")
                epoch_file.write("Configuration:\n")
                epoch_file.write(json.dumps(config, indent=4))
                epoch_file.write("\nEpoch Results:\n")
                epoch_file.write(json.dumps(server.epoch_results))
                epoch_file.write("\n\n")
            logger.info(f"Epoch results appended to {epoch_filepath}")
        except Exception as write_exc:
            logger.error(f"Failed to write simulation results to file: {write_exc}")
    except Exception as e:
        logger.error(f"Error during federated learning: {e}")
    finally:
        ray.shutdown()
            

if __name__ == '__main__':
    main()